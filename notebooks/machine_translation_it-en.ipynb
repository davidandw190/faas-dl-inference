{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAgVwChlD5ekNKqT1eiqEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidandw190/faas-dl-inference/blob/main/notebooks/machine_translation_it-en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation (IT-EN)"
      ],
      "metadata": {
        "id": "hsE5zqIkDmBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8vIEVW35EPV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers onnx onnxruntime optimum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import onnxruntime as ort\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Hs_kEUgz5MQS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-it-en'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "5YOQrepaJ1MA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Ciao, come stai?\",\n",
        "    \"Mi piace la pasta.\",\n",
        "    \"Dove si trova la stazione?\",\n",
        "    \"Che bella giornata!\",\n",
        "    \"Vorrei prenotare un tavolo per due persone, per favore.\"\n",
        "]"
      ],
      "metadata": {
        "id": "QzSUzJMj9teG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPiPfo759vCR",
        "outputId": "ac0a0413-8718-40ba-a9b1-618cd0f852d3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "WraKIkLD9wHh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing PyTorch model:\")\n",
        "for sentence in test_sentences:\n",
        "    translated = translate_text(sentence, model)\n",
        "    print(f\"Original (Italian): {sentence}\")\n",
        "    print(f\"Translated (English): {translated}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj0uSFch9zQC",
        "outputId": "35ca24d1-2b36-4f56-c826-c514755b9c0b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing PyTorch model:\n",
            "Original (Italian): Ciao, come stai?\n",
            "Translated (English): Hi, how are you?\n",
            "\n",
            "Original (Italian): Mi piace la pasta.\n",
            "Translated (English): I like pasta.\n",
            "\n",
            "Original (Italian): Dove si trova la stazione?\n",
            "Translated (English): Where is the station?\n",
            "\n",
            "Original (Italian): Che bella giornata!\n",
            "Translated (English): What a beautiful day!\n",
            "\n",
            "Original (Italian): Vorrei prenotare un tavolo per due persone, per favore.\n",
            "Translated (English): I'd like to book a table for two people, please.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ort_model = ORTModelForSeq2SeqLM.from_pretrained(model_name, from_transformers=True)\n",
        "ort_model.save_pretrained(\"onnx_model\")"
      ],
      "metadata": {
        "id": "c35Brvxq91mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_dir = \"onnx_model\"\n",
        "onnx_files = [f for f in os.listdir(onnx_model_dir) if f.endswith('.onnx')]"
      ],
      "metadata": {
        "id": "5XxWyTlv-Tyq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"machine_transaltion_it-en_quantized_model\", exist_ok=True)\n",
        "for onnx_file in onnx_files:\n",
        "    input_file = os.path.join(onnx_model_dir, onnx_file)\n",
        "    output_file = os.path.join(\"machine_transaltion_it-en_quantized_model\", f\"quantized_{onnx_file}\")\n",
        "    quantize_dynamic(input_file, output_file, weight_type=QuantType.QUInt8)\n",
        "    print(f\"Quantized {onnx_file} to {output_file}\")"
      ],
      "metadata": {
        "id": "q6GK2swf-V7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy(os.path.join(onnx_model_dir, \"config.json\"), \"machine_transaltion_it-en_quantized_model\")\n",
        "shutil.copy(os.path.join(onnx_model_dir, \"generation_config.json\"), \"machine_transaltion_it-en_quantized_model\")\n",
        "tokenizer.save_pretrained(\"machine_transaltion_it-en_quantized_model\")"
      ],
      "metadata": {
        "id": "Qh_BFbsd-XNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_onnx_files = [f for f in os.listdir(\"machine_transaltion_it-en_quantized_model\") if f.endswith('.onnx')]\n",
        "encoder_file = next((f for f in quantized_onnx_files if 'encoder' in f), None)\n",
        "decoder_file = next((f for f in quantized_onnx_files if 'decoder' in f and 'with_past' not in f), None)\n",
        "decoder_with_past_file = next((f for f in quantized_onnx_files if 'decoder' in f and 'with_past' in f), None)\n"
      ],
      "metadata": {
        "id": "39BQpTRJ-0GU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ort_quantized_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
        "    \"machine_transaltion_it-en_quantized_model\",\n",
        "    encoder_file_name=encoder_file,\n",
        "    decoder_file_name=decoder_file,\n",
        "    decoder_with_past_file_name=decoder_with_past_file if decoder_with_past_file else None\n",
        ")"
      ],
      "metadata": {
        "id": "D52SEGjO-1zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text_onnx(text, ort_model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = ort_model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Testing ONNX models:\")\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Original (Italian): {sentence}\")\n",
        "    print(f\"Translated (ONNX): {translate_text_onnx(sentence, ort_model)}\")\n",
        "    print(f\"Translated (ONNX quantized): {translate_text_onnx(sentence, ort_quantized_model)}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCdy9Hmw-4iO",
        "outputId": "2b768548-cff2-4fb0-8847-28da0155a8ce"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing ONNX models:\n",
            "Original (Italian): Ciao, come stai?\n",
            "Translated (ONNX): Hi, how are you?\n",
            "Translated (ONNX quantized): Hi, how are you?\n",
            "\n",
            "Original (Italian): Mi piace la pasta.\n",
            "Translated (ONNX): I like pasta.\n",
            "Translated (ONNX quantized): I like pasta.\n",
            "\n",
            "Original (Italian): Dove si trova la stazione?\n",
            "Translated (ONNX): Where is the station?\n",
            "Translated (ONNX quantized): Where is the station?\n",
            "\n",
            "Original (Italian): Che bella giornata!\n",
            "Translated (ONNX): What a beautiful day!\n",
            "Translated (ONNX quantized): What a beautiful day!\n",
            "\n",
            "Original (Italian): Vorrei prenotare un tavolo per due persone, per favore.\n",
            "Translated (ONNX): I'd like to book a table for two people, please.\n",
            "Translated (ONNX quantized): I'd like to book a table for two people, please.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}